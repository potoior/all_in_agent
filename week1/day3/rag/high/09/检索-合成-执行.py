import fitz  # PyMuPDFåº“ï¼Œç”¨äºå¤„ç†PDFæ–‡ä»¶
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import numpy as np  # æ•°å€¼è®¡ç®—åº“ï¼Œç”¨äºå‘é‡è¿ç®—
import json  # JSONæ•°æ®å¤„ç†
from openai import OpenAI  # OpenAI APIå®¢æˆ·ç«¯
import re  # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # HuggingFaceåµŒå…¥æ¨¡å‹

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
# base_url: SiliconFlow APIç«¯ç‚¹
# api_key: APIå¯†é’¥
client = OpenAI(
    base_url="https://api.siliconflow.cn/v1",
    api_key='sk-zqzehnidkvjxmpgoqohexqzxwnvyszxwgxucpxmtftdpgrgv'
)

def extract_text_from_pdf(pdf_path):
    """
    ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
    
    Args:
        pdf_path (str): PDFæ–‡ä»¶çš„è·¯å¾„
        
    Returns:
        str: æå–çš„æ–‡æœ¬å†…å®¹
    """
    # æ‰“å¼€PDFæ–‡ä»¶
    mypdf = fitz.open(pdf_path)
    all_text = ""

    # éå†PDFçš„æ¯ä¸€é¡µå¹¶æå–æ–‡æœ¬
    for page_num in range(mypdf.page_count):
        page = mypdf[page_num]
        text = page.get_text("text")  # è·å–é¡µé¢æ–‡æœ¬
        all_text += text

    return all_text

def chunk_text(text, chunk_size=800, overlap=0):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆæŒ‡å®šå¤§å°çš„å—
    
    RSEé€šå¸¸ä½¿ç”¨éé‡å å—ä»¥ä¾¿èƒ½å¤Ÿæ­£ç¡®é‡æ„æ®µè½
    
    Args:
        text (str): è¦åˆ†å‰²çš„åŸå§‹æ–‡æœ¬
        chunk_size (int): æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        overlap (int): ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        
    Returns:
        List[str]: æ–‡æœ¬å—åˆ—è¡¨
    """
    chunks = []

    # æŒ‰æŒ‡å®šæ­¥é•¿åˆ†å‰²æ–‡æœ¬ï¼Œæ­¥é•¿ = å—å¤§å° - é‡å å¤§å°
    # å½“overlapä¸º0æ—¶ï¼Œå—ä¹‹é—´æ²¡æœ‰é‡å 
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        # ç¡®ä¿å—ä¸ä¸ºç©º
        if chunk:
            chunks.append(chunk)

    return chunks

def create_embeddings(texts, model="BAAI/bge-base-en-v1.5"):
    """
    ä¸ºç»™å®šæ–‡æœ¬åˆ›å»ºå‘é‡åµŒå…¥
    
    Args:
        texts (str or list): å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²æˆ–æ–‡æœ¬åˆ—è¡¨
        model (str): ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º"BAAI/bge-base-en-v1.5"
        
    Returns:
        åµŒå…¥å‘é‡æˆ–åµŒå…¥å‘é‡åˆ—è¡¨
    """
    # å¦‚æœtextsä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    if not texts:
        return []

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embedding_model = HuggingFaceEmbedding(model_name=model)

    # åˆ¤æ–­å¯¹è±¡æ˜¯å¦æ˜¯listç±»å‹ å¦‚æœæ˜¯å°±æ‰¹é‡åµŒå…¥ å¦åˆ™å°±å•å¥åµŒå…¥
    if isinstance(texts, list):
        # æ‰¹é‡å¤„ç†æ–‡æœ¬åˆ—è¡¨
        response = embedding_model.get_text_embedding_batch(texts)
    else:
        # å¤„ç†å•ä¸ªæ–‡æœ¬
        response = embedding_model.get_text_embedding(texts)

    return response

class SimpleVectorStore:
    """è½»é‡çº§å‘é‡å­˜å‚¨å®ç°
    
    ç”¨äºå­˜å‚¨æ–‡æ¡£åŠå…¶å¯¹åº”çš„å‘é‡åµŒå…¥ï¼Œæ”¯æŒç›¸ä¼¼åº¦æœç´¢åŠŸèƒ½
    """
    def __init__(self, dimension=1536):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        
        Args:
            dimension (int): å‘é‡ç»´åº¦ï¼Œé»˜è®¤ä¸º1536
        """
        self.dimension = dimension  # å‘é‡ç»´åº¦
        self.vectors = []          # å­˜å‚¨å‘é‡åµŒå…¥
        self.documents = []        # å­˜å‚¨åŸå§‹æ–‡æ¡£
        self.metadata = []         # å­˜å‚¨å…ƒæ•°æ®

    def add_documents(self, documents, vectors=None, metadata=None):
        """
        å‘å‘é‡å­˜å‚¨ä¸­æ·»åŠ æ–‡æ¡£
        
        Args:
            documents (List[str]): è¦æ·»åŠ çš„æ–‡æ¡£åˆ—è¡¨
            vectors (List[array], optional): å¯¹åº”çš„å‘é‡åµŒå…¥åˆ—è¡¨
            metadata (List[dict], optional): å¯¹åº”çš„å…ƒæ•°æ®åˆ—è¡¨
        """
        # å¦‚æœæœªæä¾›å‘é‡ï¼Œåˆ™ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºä¸€ä¸ªNoneå ä½ç¬¦
        if vectors is None:
            # å¦‚æœ vectors æ˜¯ Noneï¼Œåˆ™åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸æ–‡æ¡£æ•°é‡ç›¸åŒçš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ None
            vectors = [None] * len(documents)

        # å¦‚æœæœªæä¾›å…ƒæ•°æ®ï¼Œåˆ™ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸
        if metadata is None:
            metadata = [{} for _ in range(len(documents))]

        # éå†æ–‡æ¡£ã€å‘é‡å’Œå…ƒæ•°æ®ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ°å­˜å‚¨ä¸­
        for doc, vec, meta in zip(documents, vectors, metadata):
            self.documents.append(doc)   # æ·»åŠ æ–‡æ¡£
            self.vectors.append(vec)     # æ·»åŠ å‘é‡
            self.metadata.append(meta)   # æ·»åŠ å…ƒæ•°æ®

    def search(self, query_vector, top_k=5):
        """
        åŸºäºä½™å¼¦ç›¸ä¼¼åº¦æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        
        Args:
            query_vector (array-like): æŸ¥è¯¢å‘é‡
            top_k (int): è¿”å›æœ€ç›¸å…³çš„kä¸ªç»“æœï¼Œé»˜è®¤ä¸º5
            
        Returns:
            List[Dict]: æœ€ç›¸å…³çš„æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«documentã€scoreå’Œmetadata
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£å’Œå‘é‡
        if not self.vectors or not self.documents:
            return []

        # å°†æŸ¥è¯¢å‘é‡è½¬æ¢ä¸ºnumpyæ•°ç»„
        query_array = np.array(query_vector)

        # è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ¯ä¸ªå­˜å‚¨å‘é‡çš„ç›¸ä¼¼åº¦
        similarities = []
        for i, vector in enumerate(self.vectors):
            # åªå¤„ç†éç©ºå‘é‡
            if vector is not None:
                # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ç›¸ä¼¼åº¦
                # ä½™å¼¦ç›¸ä¼¼åº¦ = (AÂ·B) / (||A|| Ã— ||B||)
                similarity = np.dot(query_array, vector) / (
                    np.linalg.norm(query_array) * np.linalg.norm(vector)
                )
                similarities.append((i, similarity))

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)

        # è·å–å‰top-kä¸ªç»“æœ
        results = []
        for i, score in similarities[:top_k]:
            results.append({
                "document": self.documents[i],     # æ–‡æ¡£å†…å®¹
                "score": float(score),             # ç›¸ä¼¼åº¦å¾—åˆ†
                "metadata": self.metadata[i]       # å…ƒæ•°æ®
            })

        return results

def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):
    """
    è®¡ç®—æ¯ä¸ªæ–‡æ¡£å—ç›¸å¯¹äºæŸ¥è¯¢çš„ä»·å€¼åˆ†æ•°
    
    é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æœç´¢è®¡ç®—æ¯ä¸ªå—çš„ç›¸å…³æ€§ï¼Œå¹¶å¯¹ä½åˆ†å—åº”ç”¨æƒ©ç½š

    Args:
        query (str): ç”¨æˆ·æŸ¥è¯¢
        chunks (List[str]): æ–‡æ¡£å—åˆ—è¡¨
        vector_store: å‘é‡å­˜å‚¨
        irrelevant_chunk_penalty (float): ä¸ç›¸å…³å—çš„æƒ©ç½šç³»æ•°ï¼Œä½äºæ­¤å€¼çš„å—ä¼šè¢«é‡åº¦æƒ©ç½š

    Returns:
        List[float]: æ¯ä¸ªå—çš„ä»·å€¼åˆ†æ•°åˆ—è¡¨
    """
    # ä¸ºæŸ¥è¯¢åˆ›å»ºåµŒå…¥å‘é‡
    query_embedding = create_embeddings(query)
    # ä½¿ç”¨å‘é‡å­˜å‚¨æœç´¢æ‰€æœ‰å—çš„ç›¸å…³æ€§å¾—åˆ†
    search_results = vector_store.search(query_embedding, top_k=len(chunks))

    # åˆ›å»ºå—ç´¢å¼•åˆ°å¾—åˆ†çš„æ˜ å°„
    chunk_scores = {}
    for result in search_results:
        """
            result = {
        "document": "è¿™æ˜¯æ–‡æ¡£å†…å®¹...",
        "score": 0.85,
        "metadata": {
            "chunk_index": 5,
            "source": "data/AI_Information.pdf"
            }
        }

        """
        chunk_index = result["metadata"]["chunk_index"]
        chunk_scores[chunk_index] = result["score"]

    # è®¡ç®—æ¯ä¸ªå—çš„ä»·å€¼ï¼ŒåŒ…æ‹¬å¯¹ä¸ç›¸å…³å—çš„æƒ©ç½š
    chunk_values = []
    for i in range(len(chunks)):
        if i in chunk_scores:
            base_score = chunk_scores[i]
            # å¯¹ä½åˆ†æ•°å—åº”ç”¨æƒ©ç½š
            # å¦‚æœå—çš„å¾—åˆ†ä½äºæƒ©ç½šé˜ˆå€¼ï¼Œåˆ™å¯¹å…¶è¿›è¡Œé‡åº¦æƒ©ç½šï¼ˆä¹˜ä»¥0.1ï¼‰
            if base_score < irrelevant_chunk_penalty:
                value = base_score * 0.1  # é‡åº¦æƒ©ç½š
            else:
                value = base_score
        else:
            # æœªæ‰¾åˆ°çš„å—ä»·å€¼ä¸º0
            value = 0.0

        chunk_values.append(value)

    return chunk_values

def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):
    """
    ä½¿ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•æ‰¾åˆ°æœ€ä½³çš„è¿ç»­æ–‡æ¡£æ®µè½
    
    é€šè¿‡è¯„ä¼°ä¸åŒæ®µè½ç»„åˆçš„ä»·å€¼ï¼Œé€‰æ‹©æ€»ä»·å€¼æœ€é«˜çš„ä¸é‡å æ®µè½é›†åˆ

    Args:
        chunk_values (List[float]): æ¯ä¸ªå—çš„ä»·å€¼åˆ†æ•°
        max_segment_length (int): å•ä¸ªæ®µè½çš„æœ€å¤§é•¿åº¦ï¼ˆå—æ•°ï¼‰
        total_max_length (int): æ‰€æœ‰æ®µè½çš„æ€»æœ€å¤§é•¿åº¦ï¼ˆå—æ•°ï¼‰
        min_segment_value (float): æ®µè½çš„æœ€å°å¹³å‡ä»·å€¼ï¼Œä½äºæ­¤å€¼çš„æ®µè½å°†è¢«å¿½ç•¥

    Returns:
        List[Tuple[int, int]]: æœ€ä½³æ®µè½çš„(èµ·å§‹ç´¢å¼•, ç»“æŸç´¢å¼•)åˆ—è¡¨
    """
    n = len(chunk_values)  # å—çš„æ€»æ•°
    segments = []  # å­˜å‚¨æ‰€æœ‰å¯èƒ½çš„æ®µè½

    # åŠ¨æ€è§„åˆ’æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„æ®µè½ç»„åˆ
    # éå†æ‰€æœ‰å¯èƒ½çš„èµ·å§‹ä½ç½®
    for start in range(n):
        # éå†æ‰€æœ‰å¯èƒ½çš„æ®µè½é•¿åº¦
        for length in range(1, min(max_segment_length + 1, n - start + 1)):
            end = start + length - 1  # è®¡ç®—ç»“æŸä½ç½®

            # è®¡ç®—æ®µè½çš„å¹³å‡ä»·å€¼
            segment_values = chunk_values[start:end + 1]
            avg_value = sum(segment_values) / len(segment_values)

            # åªä¿ç•™é«˜äºæœ€å°ä»·å€¼é˜ˆå€¼çš„æ®µè½
            if avg_value >= min_segment_value:
                segments.append({
                    'start': start,           # èµ·å§‹ç´¢å¼•
                    'end': end,              # ç»“æŸç´¢å¼•
                    'length': length,         # æ®µè½é•¿åº¦
                    'avg_value': avg_value,   # å¹³å‡ä»·å€¼
                    'total_value': sum(segment_values)  # æ€»ä»·å€¼
                })

    # æŒ‰æ€»ä»·å€¼é™åºæ’åºï¼Œä¼˜å…ˆè€ƒè™‘æ€»ä»·å€¼é«˜çš„æ®µè½
    segments.sort(key=lambda x: x['total_value'], reverse=True)

    # è´ªå¿ƒç®—æ³•é€‰æ‹©ä¸é‡å çš„æœ€ä½³æ®µè½
    selected_segments = []      # å­˜å‚¨é€‰ä¸­çš„æ®µè½
    used_chunks = set()         # è®°å½•å·²è¢«ä½¿ç”¨çš„å—
    total_length = 0            # å½“å‰é€‰ä¸­æ®µè½çš„æ€»é•¿åº¦

    # éå†æ‰€æœ‰æŒ‰ä»·å€¼æ’åºçš„æ®µè½
    for segment in segments:
        # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰æ®µè½é‡å 
        segment_chunks = set(range(segment['start'], segment['end'] + 1))
        if not segment_chunks.intersection(used_chunks):
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ€»é•¿åº¦é™åˆ¶
            if total_length + segment['length'] <= total_max_length:
                # æ·»åŠ æ®µè½åˆ°é€‰ä¸­åˆ—è¡¨
                selected_segments.append((segment['start'], segment['end']))
                # æ›´æ–°å·²ä½¿ç”¨çš„å—é›†åˆ
                used_chunks.update(segment_chunks)
                # æ›´æ–°æ€»é•¿åº¦
                total_length += segment['length']

    # æŒ‰èµ·å§‹ä½ç½®æ’åºä»¥ä¿æŒæ–‡æ¡£é¡ºåº
    selected_segments.sort(key=lambda x: x[0])

    return selected_segments

def reconstruct_segments(chunks, best_segments):
    """
    æ ¹æ®æœ€ä½³æ®µè½ç´¢å¼•é‡æ„å®Œæ•´çš„æ–‡æ¡£æ®µè½
    
    å°†è¿ç»­çš„æ–‡æ¡£å—åˆå¹¶æˆå®Œæ•´çš„æ®µè½ï¼Œä»¥æä¾›æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡

    Args:
        chunks (List[str]): åŸå§‹æ–‡æ¡£å—åˆ—è¡¨
        best_segments (List[Tuple[int, int]]): æœ€ä½³æ®µè½çš„ç´¢å¼•èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(èµ·å§‹ç´¢å¼•, ç»“æŸç´¢å¼•)

    Returns:
        List[str]: é‡æ„åçš„æ–‡æ¡£æ®µè½åˆ—è¡¨
    """
    reconstructed_segments = []

    # éå†æ‰€æœ‰é€‰ä¸­çš„æ®µè½
    for start, end in best_segments:
        # åˆå¹¶è¿ç»­çš„å—å½¢æˆå®Œæ•´æ®µè½
        # ä½¿ç”¨ç©ºæ ¼è¿æ¥ç›¸é‚»çš„å—
        segment_text = " ".join(chunks[start:end + 1])
        reconstructed_segments.append(segment_text)

    return reconstructed_segments

def format_segments_for_context(segments):
    """
    æ ¼å¼åŒ–æ®µè½ç”¨äºä½œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡
    
    ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ ç¼–å·æ ‡ç­¾ï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“åŒºåˆ†ä¸åŒçš„æ®µè½

    Args:
        segments (List[str]): é‡æ„çš„æ–‡æ¡£æ®µè½åˆ—è¡¨

    Returns:
        str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œæ®µè½ä¹‹é—´ç”¨åŒæ¢è¡Œåˆ†éš”
    """
    formatted_context = []

    # éå†æ‰€æœ‰æ®µè½ï¼Œä¸ºæ¯ä¸ªæ®µè½æ·»åŠ ç¼–å·
    for i, segment in enumerate(segments, 1):
        # æ ¼å¼åŒ–æ¯ä¸ªæ®µè½ï¼šæ®µè½ç¼–å· + æ¢è¡Œ + æ®µè½å†…å®¹
        formatted_context.append(f"æ®µè½{i}:\n{segment}")

    # ä½¿ç”¨åŒæ¢è¡Œç¬¦è¿æ¥æ‰€æœ‰æ®µè½
    return "\n\n".join(formatted_context)

def generate_response(query, context, model="Qwen/Qwen2.5-72B-Instruct"):
    """
    åŸºäºç»™å®šä¸Šä¸‹æ–‡ç”Ÿæˆå¯¹æŸ¥è¯¢çš„å›ç­”
    
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”

    Args:
        query (str): ç”¨æˆ·æŸ¥è¯¢
        context (str): æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡
        model (str): ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé»˜è®¤ä¸º"Qwen/Qwen2.5-72B-Instruct"
        
    Returns:
        str: AIç”Ÿæˆçš„å›ç­”
    """
    # ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰AIåŠ©æ‰‹çš„è¡Œä¸ºå‡†åˆ™
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä¸¥æ ¼åŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­å¾—å‡ºç­”æ¡ˆï¼Œè¯·å›ç­”ï¼š'æˆ‘æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'"

    # ç”¨æˆ·æç¤ºè¯ - å°†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·æŸ¥è¯¢ç»„åˆæˆå®Œæ•´çš„æç¤º
    user_prompt = f"""
    ä¸Šä¸‹æ–‡:
    {context}

    é—®é¢˜: {query}

    è¯·åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
    """

    # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆå›ç­”
    response = client.chat.completions.create(
        model=model,           # ä½¿ç”¨çš„æ¨¡å‹
        temperature=0,         # è®¾ç½®ä¸º0ä»¥è·å¾—ç¡®å®šæ€§å›ç­”
        messages=[
            {"role": "system", "content": system_prompt},   # ç³»ç»Ÿæç¤º
            {"role": "user", "content": user_prompt}       # ç”¨æˆ·æç¤º
        ]
    )

    # è¿”å›æ¨¡å‹ç”Ÿæˆçš„å›ç­”å†…å®¹
    return response.choices[0].message.content

def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):
    """
    ä½¿ç”¨æ£€ç´¢-åˆæˆ-æ‰§è¡Œ(RSE)æ¶æ„çš„å®Œæ•´RAGæµç¨‹
    
    å®ç°äº†å®Œæ•´çš„RSEæµç¨‹ï¼šæ–‡æ¡£å¤„ç† -> å—ä»·å€¼è®¡ç®— -> æœ€ä½³æ®µè½æŸ¥æ‰¾ -> æ®µè½é‡æ„ -> å›ç­”ç”Ÿæˆ

    Args:
        pdf_path (str): PDFæ–‡æ¡£è·¯å¾„
        query (str): ç”¨æˆ·æŸ¥è¯¢
        chunk_size (int): æ–‡æ¡£å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        irrelevant_chunk_penalty (float): ä¸ç›¸å…³å—æƒ©ç½šç³»æ•°ï¼Œä½äºæ­¤å€¼çš„å—ä¼šè¢«é‡åº¦æƒ©ç½š

    Returns:
        dict: åŒ…å«RSEå¤„ç†ç»“æœçš„å­—å…¸
    """
    print("å¼€å§‹RSEå¤„ç†æµç¨‹...")

    # 1. å¤„ç†æ–‡æ¡£
    # æå–æ–‡æœ¬ã€åˆ†å—ã€åˆ›å»ºåµŒå…¥å‘é‡å¹¶å»ºç«‹å‘é‡å­˜å‚¨
    print("1. å¤„ç†æ–‡æ¡£...")
    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)

    # 2. è®¡ç®—å—ä»·å€¼
    # åŸºäºæŸ¥è¯¢è®¡ç®—æ¯ä¸ªæ–‡æ¡£å—çš„ç›¸å…³æ€§ä»·å€¼
    print("2. è®¡ç®—å—ä»·å€¼...")
    chunk_values = calculate_chunk_values(
        query, chunks, vector_store, irrelevant_chunk_penalty
    )

    # è¾“å‡ºå—ä»·å€¼çš„ç»Ÿè®¡ä¿¡æ¯
    print(f"å—ä»·å€¼åˆ†å¸ƒ: æœ€é«˜={max(chunk_values):.3f}, æœ€ä½={min(chunk_values):.3f}, å¹³å‡={np.mean(chunk_values):.3f}")

    # 3. æ‰¾åˆ°æœ€ä½³æ®µè½
    # ä½¿ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•æ‰¾åˆ°æœ€æœ‰ä»·å€¼çš„ä¸é‡å æ®µè½ç»„åˆ
    print("3. å¯»æ‰¾æœ€ä½³æ®µè½...")
    best_segments = find_best_segments(
        chunk_values,
        max_segment_length=20,      # å•ä¸ªæ®µè½æœ€å¤šåŒ…å«20ä¸ªå—
        total_max_length=30,        # æ‰€æœ‰æ®µè½æ€»è®¡æœ€å¤šåŒ…å«30ä¸ªå—
        min_segment_value=0.2       # æ®µè½çš„æœ€å°å¹³å‡ä»·å€¼
    )

    # è¾“å‡ºæ‰¾åˆ°çš„æœ€ä½³æ®µè½ä¿¡æ¯
    print(f"æ‰¾åˆ° {len(best_segments)} ä¸ªæœ€ä½³æ®µè½")
    for i, (start, end) in enumerate(best_segments):
        print(f"  æ®µè½{i+1}: å—{start}-{end} (é•¿åº¦: {end-start+1})")

    # 4. é‡æ„æ®µè½
    # å°†é€‰ä¸­çš„å—åˆå¹¶æˆå®Œæ•´çš„æ®µè½
    print("4. é‡æ„æ–‡æ¡£æ®µè½...")
    reconstructed_segments = reconstruct_segments(chunks, best_segments)

    # 5. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
    # å°†é‡æ„çš„æ®µè½æ ¼å¼åŒ–ä¸ºæ¨¡å‹å‹å¥½çš„ä¸Šä¸‹æ–‡
    context = format_segments_for_context(reconstructed_segments)

    # è¾“å‡ºä¸Šä¸‹æ–‡é•¿åº¦ä¿¡æ¯
    print(f"ç”Ÿæˆçš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

    # 6. ç”Ÿæˆå›ç­”
    # ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
    print("5. ç”Ÿæˆå›ç­”...")
    response = generate_response(query, context)

    # è¿”å›å®Œæ•´çš„å¤„ç†ç»“æœ
    return {
        "query": query,                          # ç”¨æˆ·æŸ¥è¯¢
        "total_chunks": len(chunks),             # æ€»å—æ•°
        "chunk_values": chunk_values,            # æ¯ä¸ªå—çš„ä»·å€¼
        "best_segments": best_segments,          # æœ€ä½³æ®µè½
        "reconstructed_segments": reconstructed_segments,  # é‡æ„çš„æ®µè½
        "context": context,                      # æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡
        "response": response,                    # ç”Ÿæˆçš„å›ç­”
        "doc_info": doc_info                     # æ–‡æ¡£ä¿¡æ¯
    }

def process_document(pdf_path, chunk_size=800):
    """
    å¤„ç†æ–‡æ¡£ç”¨äºRSEæµç¨‹
    
    åŒ…æ‹¬æå–æ–‡æœ¬ã€åˆ†å—ã€åˆ›å»ºåµŒå…¥å‘é‡å’Œå»ºç«‹å‘é‡å­˜å‚¨ç­‰æ­¥éª¤

    Args:
        pdf_path (str): PDFæ–‡æ¡£è·¯å¾„
        chunk_size (int): æ–‡æ¡£å—å¤§å°

    Returns:
        Tuple[List[str], SimpleVectorStore, Dict]: æ–‡æ¡£å—åˆ—è¡¨ã€å‘é‡å­˜å‚¨å®ä¾‹å’Œæ–‡æ¡£ä¿¡æ¯å­—å…¸
    """
    print("æ­£åœ¨ä»æ–‡æ¡£æå–æ–‡æœ¬...")
    # ä»PDFæ–‡ä»¶ä¸­æå–å…¨éƒ¨æ–‡æœ¬å†…å®¹
    text = extract_text_from_pdf(pdf_path)

    print("æ­£åœ¨åˆ†å—æ–‡æœ¬ä¸ºéé‡å æ®µè½...")
    # å°†æ–‡æœ¬åˆ†å‰²æˆéé‡å çš„å—
    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)
    print(f"åˆ›å»ºäº† {len(chunks)} ä¸ªå—")

    print("æ­£åœ¨ä¸ºå—ç”ŸæˆåµŒå…¥...")
    # ä¸ºæ¯ä¸ªæ–‡æœ¬å—åˆ›å»ºå‘é‡åµŒå…¥
    chunk_embeddings = create_embeddings(chunks)

    # åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹
    vector_store = SimpleVectorStore()

    # ä¸ºæ¯ä¸ªå—åˆ›å»ºå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬å—ç´¢å¼•å’Œæºæ–‡ä»¶è·¯å¾„
    # å—ç´¢å¼•ç”¨äºåç»­çš„æ®µè½é‡æ„
    metadata = [{"chunk_index": i, "source": pdf_path} for i in range(len(chunks))]
    # å°†æ–‡æ¡£å—ã€åµŒå…¥å‘é‡å’Œå…ƒæ•°æ®æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
    vector_store.add_documents(chunks, chunk_embeddings, metadata)

    # è·Ÿè¸ªåŸå§‹æ–‡æ¡£ç»“æ„ç”¨äºæ®µè½é‡æ„
    doc_info = {
        "total_chunks": len(chunks),      # æ€»å—æ•°
        "chunk_size": chunk_size,         # å—å¤§å°
        "total_characters": len(text),    # æ€»å­—ç¬¦æ•°
        "source": pdf_path               # æºæ–‡ä»¶è·¯å¾„
    }

    return chunks, vector_store, doc_info

## å®é™…åº”ç”¨ç¤ºä¾‹


# RSEå®Œæ•´æµç¨‹æ¼”ç¤º
pdf_path = "data/AI_Information.pdf"
query = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ çš„ä¸»è¦ç‰¹ç‚¹ï¼Ÿ"

print(f"æŸ¥è¯¢: {query}")
print("="*60)

# æ‰§è¡ŒRSEæµç¨‹
rse_result = rag_with_rse(
    pdf_path=pdf_path,
    query=query,
    chunk_size=800,
    irrelevant_chunk_penalty=0.2
)

# æ˜¾ç¤ºè¯¦ç»†ç»“æœ
print(f"\nğŸ“Š RSEå¤„ç†ç»“æœ:")
print(f"- æ€»æ–‡æ¡£å—æ•°: {rse_result['total_chunks']}")
print(f"- é€‰æ‹©çš„æ®µè½æ•°: {len(rse_result['best_segments'])}")
print(f"- ä¸Šä¸‹æ–‡æ€»é•¿åº¦: {len(rse_result['context'])} å­—ç¬¦")

print(f"\nğŸ¯ é€‰æ‹©çš„æ®µè½:")
for i, (start, end) in enumerate(rse_result['best_segments']):
    avg_value = np.mean(rse_result['chunk_values'][start:end+1])
    print(f"æ®µè½{i+1}: å—{start}-{end}, å¹³å‡ä»·å€¼: {avg_value:.3f}")

print(f"\nğŸ“ é‡æ„çš„æ®µè½é¢„è§ˆ:")
for i, segment in enumerate(rse_result['reconstructed_segments']):
    print(f"\næ®µè½{i+1} (å‰200å­—ç¬¦):")
    print(segment[:200] + "...")

print(f"\nğŸ¤– ç”Ÿæˆçš„å›ç­”:")
print(rse_result['response'])

# ä¸æ ‡å‡†RAGå¯¹æ¯”
print(f"\n" + "="*60)
print("RSE vs æ ‡å‡†RAGå¯¹æ¯”:")
print("="*60)

# æ ‡å‡†RAG
# æ³¨æ„ï¼šè¿™éƒ¨åˆ†ä»£ç å¼•ç”¨äº†æœªå®šä¹‰çš„å‡½æ•°ï¼Œå®é™…è¿è¡Œæ—¶ä¼šæŠ¥é”™
# standard_result = standard_top_k_retrieval(pdf_path, query, k=10)
print(f"\næ ‡å‡†RAG:")
print(f"- æ£€ç´¢å—æ•°: {len(standard_result['results'])}")
print(f"- ä¸Šä¸‹æ–‡é•¿åº¦: {len(standard_result['context'])} å­—ç¬¦")
print(f"- å›ç­”: {standard_result['response'][:200]}...")

print(f"\nRSE:")
print(f"- æ™ºèƒ½æ®µè½æ•°: {len(rse_result['best_segments'])}")
print(f"- ä¸Šä¸‹æ–‡é•¿åº¦: {len(rse_result['context'])} å­—ç¬¦")
print(f"- å›ç­”: {rse_result['response'][:200]}...")

# ç¨‹åºç»“æŸ
