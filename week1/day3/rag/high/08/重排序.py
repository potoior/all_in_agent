import fitz
import os
import numpy as np
import json
from openai import OpenAI
import re
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# 初始化OpenAI客户端
client = OpenAI(
    base_url="https://api.siliconflow.cn/v1",
    api_key='sk-zqzehnidkvjxmpgoqohexqzxwnvyszxwgxucpxmtftdpgrgv'
)

def extract_text_from_pdf(pdf_path):
    """从PDF文件中提取文本"""
    mypdf = fitz.open(pdf_path)
    all_text = ""

    for page_num in range(mypdf.page_count):
        page = mypdf[page_num]
        text = page.get_text("text")
        all_text += text

    return all_text

def chunk_text(text, n, overlap):
    """将文本分割成重叠的块"""
    chunks = []

    for i in range(0, len(text), n - overlap):
        chunks.append(text[i:i + n])

    return chunks

class SimpleVectorStore:
    """简单的向量存储实现"""
    def __init__(self):
        self.vectors = []
        self.texts = []
        self.metadata = []

    def add_item(self, text, embedding, metadata=None):
        """向向量存储添加项目"""
        self.vectors.append(np.array(embedding))
        self.texts.append(text)
        self.metadata.append(metadata or {})

    def similarity_search(self, query_embedding, k=5):
        """查找最相似的项目"""
        if not self.vectors:
            return []

        query_vector = np.array(query_embedding)

        # 使用余弦相似度计算相似性
        similarities = []
        for i, vector in enumerate(self.vectors):
            similarity = np.dot(query_vector, vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(vector)
            )
            similarities.append((i, similarity))

        # 按相似度降序排序
        similarities.sort(key=lambda x: x[1], reverse=True)

        # 返回top k结果
        results = []
        for i in range(min(k, len(similarities))):
            idx, score = similarities[i]
            results.append({
                "text": self.texts[idx],
                "metadata": self.metadata[idx],
                "similarity": score
            })

        return results

def create_embeddings(text, model="BAAI/bge-base-en-v1.5"):
    """为给定文本创建嵌入向量"""
    embedding_model = HuggingFaceEmbedding(model_name=model)

    if isinstance(text, list):
        response = embedding_model.get_text_embedding_batch(text)
    else:
        response = embedding_model.get_text_embedding(text)

    return response

def rerank_with_llm(query, results, top_n=3, model="Qwen/Qwen2.5-72B-Instruct"):
    """
    使用LLM相关性评分对搜索结果进行重排序

    Args:
        query (str): 用户查询
        results (List[Dict]): 初始搜索结果
        top_n (int): 重排序后返回的结果数
        model (str): 用于评分的模型

    Returns:
        List[Dict]: 重排序后的结果
    """
    if len(results) <= top_n:
        return results

    scored_results = []

    for i, result in enumerate(results):
        # 为每个结果生成相关性评分
        scoring_prompt = f"""
        请评估以下文档与用户查询的相关性。

        用户查询: {query}

        文档内容: {result['text'][:500]}...

        请给出0-10的相关性评分（10分最相关）。
        只返回数字分数，不要其他内容。
        """

        try:
            response = client.chat.completions.create(
                model=model,
                temperature=0,
                messages=[
                    {"role": "system", "content": "你是一个专业的文档相关性评估专家。"},
                    {"role": "user", "content": scoring_prompt}
                ]
            )

            # 提取评分
            score_text = response.choices[0].message.content.strip()
            score = float(re.search(r'\d+\.?\d*', score_text).group())

            # 添加到结果中
            result['llm_score'] = score
            scored_results.append(result)

        except Exception as e:
            print(f"评分结果 {i} 时出错: {e}")
            result['llm_score'] = 0
            scored_results.append(result)

    # 按LLM评分降序排序
    scored_results.sort(key=lambda x: x['llm_score'], reverse=True)

    return scored_results[:top_n]

def rerank_with_keywords(query, results, top_n=3):
    """
    基于关键词匹配对搜索结果进行重排序

    Args:
        query (str): 用户查询
        results (List[Dict]): 初始搜索结果
        top_n (int): 重排序后返回的结果数

    Returns:
        List[Dict]: 重排序后的结果
    """
    # 提取查询中的关键词
    query_keywords = set(re.findall(r'\b\w+\b', query.lower()))

    scored_results = []

    for result in results:
        text_lower = result['text'].lower()
        text_keywords = set(re.findall(r'\b\w+\b', text_lower))

        # 计算关键词匹配度
        # intersection返回交集
        common_keywords = query_keywords.intersection(text_keywords)
        keyword_score = len(common_keywords) / len(query_keywords) if query_keywords else 0

        # 计算关键词在文档中的频率
        keyword_frequency = sum(text_lower.count(keyword) for keyword in common_keywords)

        # 计算文档长度权重（较短文档可能更相关）
        length_penalty = 1 / (1 + len(result['text']) / 1000)

        # 综合评分：原始相似度 + 关键词匹配 + 频率 + 长度权重
        combined_score = (
            result['similarity'] * 0.4 +
            keyword_score * 0.3 +
            min(keyword_frequency / 10, 0.2) +  # 限制频率权重
            length_penalty * 0.1
        )

        result['keyword_score'] = combined_score
        scored_results.append(result)

    # 按综合评分降序排序
    scored_results.sort(key=lambda x: x['keyword_score'], reverse=True)

    return scored_results[:top_n]

def generate_response(query, context, model="Qwen/Qwen2.5-72B-Instruct"):
    """基于上下文生成回答"""
    system_prompt = "你是一个AI助手，严格基于给定的上下文回答问题。如果无法从提供的上下文中得出答案，请回答：'我没有足够的信息来回答这个问题。'"

    user_prompt = f"""
    上下文:
    {context}

    问题: {query}

    请基于以上上下文回答问题。
    """

    response = client.chat.completions.create(
        model=model,
        temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )

    return response.choices[0].message.content

def rag_with_reranking(query, vector_store, reranking_method="llm", top_n=3, model="Qwen/Qwen2.5-72B-Instruct"):
    """
    使用重排序的RAG系统

    Args:
        query (str): 用户查询
        vector_store: 向量存储
        reranking_method (str): 重排序方法 ("llm", "keywords", "none")
        top_n (int): 最终返回的结果数
        model (str): 使用的模型

    Returns:
        dict: 包含重排序结果和生成回答的字典
    """
    # 1. 初始检索（获取更多候选结果）
    initial_k = max(top_n * 2, 10)  # 获取2倍的候选结果
    query_embedding = create_embeddings(query)
    initial_results = vector_store.similarity_search(query_embedding, initial_k)

    print(f"初始检索到 {len(initial_results)} 个结果")

    # 2. 应用重排序
    if reranking_method == "llm":
        print("应用LLM重排序...")
        reranked_results = rerank_with_llm(query, initial_results, top_n, model)
    elif reranking_method == "keywords":
        print("应用关键词重排序...")
        reranked_results = rerank_with_keywords(query, initial_results, top_n)
    else:
        print("不使用重排序...")
        reranked_results = initial_results[:top_n]

    # 3. 准备上下文
    context = "\n\n".join([
        f"段落{i+1}: {result['text']}"
        for i, result in enumerate(reranked_results)
    ])

    # 4. 生成回答
    response = generate_response(query, context, model)

    return {
        "query": query,
        "reranking_method": reranking_method,
        "initial_results_count": len(initial_results),
        "reranked_results": reranked_results,
        "context": context,
        "response": response
    }


# 实际应用
# 处理文档并建立向量存储
pdf_path = "data/AI_Information.pdf"
print("处理文档...")

# 提取和分块
text = extract_text_from_pdf(pdf_path)
chunks = chunk_text(text, 1000, 200)
print(f"创建了 {len(chunks)} 个文本块")

# 创建嵌入和向量存储
embeddings = create_embeddings(chunks)
vector_store = SimpleVectorStore()

for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
    vector_store.add_item(
        text=chunk,
        embedding=embedding,
        metadata={"index": i, "source": pdf_path}
    )

print(f"向量存储已建立，包含 {len(chunks)} 个块")

# 加载测试查询
with open('data/val.json') as f:
    data = json.load(f)

query = data[0]['question']
print(f"\n测试查询: {query}")

# 比较不同重排序方法
methods = ["none", "keywords", "llm"]
results = {}

for method in methods:
    print(f"\n{'='*20} {method.upper()} 方法 {'='*20}")

    result = rag_with_reranking(
        query=query,
        vector_store=vector_store,
        reranking_method=method,
        top_n=3
    )

    results[method] = result

    print(f"重排序后的结果:")
    for i, res in enumerate(result['reranked_results'], 1):
        print(f"\n结果 {i}:")
        if method == "llm":
            print(f"LLM评分: {res.get('llm_score', 'N/A')}")
        elif method == "keywords":
            print(f"关键词评分: {res.get('keyword_score', 'N/A'):.4f}")
        print(f"原始相似度: {res['similarity']:.4f}")
        print(f"内容: {res['text'][:150]}...")

    print(f"\n生成的回答:")
    print(result['response'])

# 显示对比总结
print(f"\n{'='*60}")
print("重排序方法对比总结:")
print("="*60)

for method, result in results.items():
    print(f"\n【{method.upper()}方法】")
    print(f"回答: {result['response'][:200]}...")