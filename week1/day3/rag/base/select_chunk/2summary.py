import fitz
import os
import numpy as np
import json
from openai import OpenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œç”¨äºåç»­ç”Ÿæˆå›ç­”
# base_url: SiliconFlow APIç«¯ç‚¹
# api_key: ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
client = OpenAI(
    base_url="https://api.siliconflow.cn/v1",
   api_key=os.getenv("OPENROUTER_API_KEY")
)

def extract_text_from_pdf(pdf_path):
    """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    # æ‰“å¼€PDFæ–‡ä»¶
    mypdf = fitz.open(pdf_path)
    all_text = ""

    # éå†æ¯ä¸€é¡µå¹¶æå–æ–‡æœ¬
    for page_num in range(mypdf.page_count):
        page = mypdf[page_num]
        text = page.get_text("text")
        all_text += text

    return all_text

def analyze_document_characteristics(text):
    """
    åˆ†ææ–‡æ¡£ç‰¹å¾ä»¥ç¡®å®šæœ€ä¼˜åˆ†å—å¤§å°

    Args:
        text (str): æ–‡æ¡£æ–‡æœ¬

    Returns:
        dict: æ–‡æ¡£ç‰¹å¾åˆ†æç»“æœ
    """
    # åŸºç¡€ç»Ÿè®¡
    total_length = len(text)
    sentences = text.split('.')  # æŒ‰å¥å·åˆ†å‰²å¥å­
    paragraphs = text.split('\n\n')  # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½

    # è®¡ç®—ç‰¹å¾
    # avg_sentence_length: å¹³å‡å¥å­é•¿åº¦ï¼Œç”¨äºè¯„ä¼°å¥å­å¤æ‚åº¦
    # é€šè¿‡åˆ—è¡¨æ¨å¯¼å¼è¿‡æ»¤æ‰ç©ºå¥å­ï¼Œè®¡ç®—æ¯ä¸ªå¥å­å»é™¤é¦–å°¾ç©ºç™½åçš„é•¿åº¦ï¼Œå†æ±‚å¹³å‡å€¼
    avg_sentence_length = np.mean([len(s.strip()) for s in sentences if s.strip()])
    
    # avg_paragraph_length: å¹³å‡æ®µè½é•¿åº¦ï¼Œç”¨äºè¯„ä¼°æ–‡æ¡£ç»“æ„
    # é€šè¿‡åˆ—è¡¨æ¨å¯¼å¼è¿‡æ»¤æ‰ç©ºæ®µè½ï¼Œè®¡ç®—æ¯ä¸ªæ®µè½å»é™¤é¦–å°¾ç©ºç™½åçš„é•¿åº¦ï¼Œå†æ±‚å¹³å‡å€¼
    avg_paragraph_length = np.mean([len(p.strip()) for p in paragraphs if p.strip()])

    # ä¿¡æ¯å¯†åº¦åˆ†æ
    # unique_words: å”¯ä¸€è¯æ±‡æ•°é‡
    # total_words: æ€»è¯æ±‡æ•°é‡
    # vocabulary_richness: è¯æ±‡ä¸°å¯Œåº¦ = å”¯ä¸€è¯æ±‡æ•° / æ€»è¯æ±‡æ•°
    unique_words = len(set(text.lower().split()))
    total_words = len(text.split())
    vocabulary_richness = unique_words / total_words if total_words > 0 else 0

    # ç»“æ„å¤æ‚åº¦
    # line_breaks: æ¢è¡Œç¬¦æ•°é‡
    # structural_complexity: ç»“æ„å¤æ‚åº¦ = æ¢è¡Œç¬¦æ•° / æ–‡æ¡£æ€»é•¿åº¦
    line_breaks = text.count('\n')
    structural_complexity = line_breaks / total_length if total_length > 0 else 0

    characteristics = {
        'total_length': total_length,
        'avg_sentence_length': avg_sentence_length,
        'avg_paragraph_length': avg_paragraph_length,
        'vocabulary_richness': vocabulary_richness,
        'structural_complexity': structural_complexity,
        'sentence_count': len([s for s in sentences if s.strip()]),
        'paragraph_count': len([p for p in paragraphs if p.strip()])
    }

    return characteristics

def analyze_query_characteristics(query):
    """
    åˆ†ææŸ¥è¯¢ç‰¹å¾

    Args:
        query (str): ç”¨æˆ·æŸ¥è¯¢

    Returns:
        dict: æŸ¥è¯¢ç‰¹å¾åˆ†æç»“æœ
    """
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    query_length = len(query)
    word_count = len(query.split())
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç–‘é—®è¯
    question_words = ['what', 'how', 'why', 'when', 'where', 'who', 'which']
    has_question_words = any(word.lower() in query.lower() for word in question_words)

    # æŸ¥è¯¢å¤æ‚åº¦è¯„ä¼°
    # é€šè¿‡æ£€æµ‹è¿æ¥è¯å’Œå¤æ‚æ¦‚å¿µè¯æ¥è¯„ä¼°æŸ¥è¯¢å¤æ‚åº¦
    complexity_indicators = ['and', 'or', 'compare', 'difference', 'relationship', 'impact']
    complexity_score = sum(1 for indicator in complexity_indicators if indicator in query.lower())

    return {
        'query_length': query_length,
        'word_count': word_count,
        'has_question_words': has_question_words,
        'complexity_score': complexity_score,
        'is_specific': word_count <= 5,      # ç®€å•æŸ¥è¯¢ï¼šè¯æ•°<=5
        'is_complex': complexity_score >= 2  # å¤æ‚æŸ¥è¯¢ï¼šå¤æ‚åº¦>=2
    }

def recommend_chunk_size(doc_characteristics, query_characteristics):
    """
    åŸºäºæ–‡æ¡£å’ŒæŸ¥è¯¢ç‰¹å¾æ¨èæœ€ä¼˜åˆ†å—å¤§å°

    Args:
        doc_characteristics (dict): æ–‡æ¡£ç‰¹å¾
        query_characteristics (dict): æŸ¥è¯¢ç‰¹å¾

    Returns:
        tuple: (æ¨èçš„åˆ†å—å¤§å°, é‡å å¤§å°, æ¨èç†ç”±)
    """
    # åŸºå‡†åˆ†å—å¤§å°
    base_chunk_size = 1000

    # æ ¹æ®æ–‡æ¡£ç‰¹å¾è°ƒæ•´
    if doc_characteristics['avg_paragraph_length'] > 500:
        # æ®µè½è¾ƒé•¿çš„æ–‡æ¡£ï¼Œä½¿ç”¨è¾ƒå¤§çš„åˆ†å—
        doc_adjustment = 1.3
        reason = "æ–‡æ¡£æ®µè½è¾ƒé•¿ï¼Œ"
    elif doc_characteristics['avg_paragraph_length'] < 200:
        # æ®µè½è¾ƒçŸ­çš„æ–‡æ¡£ï¼Œä½¿ç”¨è¾ƒå°çš„åˆ†å—
        doc_adjustment = 0.7
        reason = "æ–‡æ¡£æ®µè½è¾ƒçŸ­ï¼Œ"
    else:
        doc_adjustment = 1.0
        reason = "æ–‡æ¡£ç»“æ„é€‚ä¸­ï¼Œ"

    # æ ¹æ®è¯æ±‡ä¸°å¯Œåº¦è°ƒæ•´
    if doc_characteristics['vocabulary_richness'] > 0.7:
        vocab_adjustment = 1.2  # è¯æ±‡ä¸°å¯Œï¼Œéœ€è¦æ›´å¤§çš„ä¸Šä¸‹æ–‡
        reason += "è¯æ±‡ä¸°å¯Œï¼Œ"
    elif doc_characteristics['vocabulary_richness'] < 0.4:
        vocab_adjustment = 0.8  # è¯æ±‡å•ä¸€ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå°åˆ†å—
        reason += "è¯æ±‡ç›¸å¯¹å•ä¸€ï¼Œ"
    else:
        vocab_adjustment = 1.0
        reason += "è¯æ±‡å¯†åº¦é€‚ä¸­ï¼Œ"

    # æ ¹æ®æŸ¥è¯¢ç‰¹å¾è°ƒæ•´
    if query_characteristics['is_complex']:
        query_adjustment = 1.4  # å¤æ‚æŸ¥è¯¢éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
        reason += "æŸ¥è¯¢å¤æ‚éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼Œ"
    elif query_characteristics['is_specific']:
        query_adjustment = 0.8  # å…·ä½“æŸ¥è¯¢å¯ä»¥ä½¿ç”¨è¾ƒå°åˆ†å—
        reason += "æŸ¥è¯¢å…·ä½“å¯ä½¿ç”¨è¾ƒå°åˆ†å—ï¼Œ"
    else:
        query_adjustment = 1.0
        reason += "æŸ¥è¯¢å¤æ‚åº¦é€‚ä¸­ï¼Œ"

    # è®¡ç®—æœ€ç»ˆåˆ†å—å¤§å°
    final_chunk_size = int(base_chunk_size * doc_adjustment * vocab_adjustment * query_adjustment)

    # ç¡®ä¿åˆ†å—å¤§å°åœ¨åˆç†èŒƒå›´å†…ï¼ˆ400-2000å­—ç¬¦ï¼‰
    final_chunk_size = max(400, min(2000, final_chunk_size))

    # è®¡ç®—é‡å å¤§å°ï¼ˆé€šå¸¸ä¸ºåˆ†å—å¤§å°çš„20%ï¼‰
    overlap_size = int(final_chunk_size * 0.2)

    reason += f"æ¨èåˆ†å—å¤§å°ä¸º{final_chunk_size}å­—ç¬¦"

    return final_chunk_size, overlap_size, reason

def create_chunks_with_size(text, chunk_size, overlap_size):
    """
    ä½¿ç”¨æŒ‡å®šå¤§å°åˆ›å»ºæ–‡æœ¬åˆ†å—

    Args:
        text (str): è¦åˆ†å—çš„æ–‡æœ¬
        chunk_size (int): åˆ†å—å¤§å°
        overlap_size (int): é‡å å¤§å°

    Returns:
        List[str]: æ–‡æ¡£åˆ†å—åˆ—è¡¨
    """
    chunks = []

    # æŒ‰ç…§æ­¥é•¿éå†æ–‡æœ¬åˆ›å»ºåˆ†å—
    # æ­¥é•¿ = åˆ†å—å¤§å° - é‡å å¤§å°ï¼Œç¡®ä¿ç›¸é‚»åˆ†å—æœ‰æŒ‡å®šå¤§å°çš„é‡å 
    for i in range(0, len(text), chunk_size - overlap_size):
        chunk = text[i:i + chunk_size]
        # åªä¿ç•™éç©ºåˆ†å—
        if chunk.strip():
            chunks.append(chunk)

    return chunks

def create_embeddings(text, model="BAAI/bge-base-en-v1.5"):
    """ä¸ºç»™å®šæ–‡æœ¬åˆ›å»ºåµŒå…¥å‘é‡"""
    # åˆå§‹åŒ–HuggingFaceåµŒå…¥æ¨¡å‹
    embedding_model = HuggingFaceEmbedding(model_name=model)

    # æ ¹æ®è¾“å…¥ç±»å‹åˆ›å»ºåµŒå…¥å‘é‡
    if isinstance(text, list):
        # æ‰¹é‡å¤„ç†æ–‡æœ¬åˆ—è¡¨
        response = embedding_model.get_text_embedding_batch(text)
    else:
        # å¤„ç†å•ä¸ªæ–‡æœ¬
        response = embedding_model.get_text_embedding(text)

    return response

def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def search_with_chunks(query, chunks, embeddings, top_k=5):
    """
    ä½¿ç”¨ç»™å®šçš„åˆ†å—å’ŒåµŒå…¥è¿›è¡Œæœç´¢

    Args:
        query (str): æŸ¥è¯¢
        chunks (List[str]): æ–‡æ¡£åˆ†å—
        embeddings (List): åµŒå…¥å‘é‡
        top_k (int): è¿”å›çš„ç»“æœæ•°é‡

    Returns:
        List[Dict]: æœç´¢ç»“æœ
    """
    # ä¸ºæŸ¥è¯¢åˆ›å»ºåµŒå…¥å‘é‡
    query_embedding = create_embeddings(query)
    similarities = []

    # è®¡ç®—æŸ¥è¯¢ä¸æ¯ä¸ªæ–‡æ¡£åˆ†å—çš„ç›¸ä¼¼åº¦
    for i, chunk_embedding in enumerate(embeddings):
        similarity = cosine_similarity(
            np.array(query_embedding),
            np.array(chunk_embedding)
        )
        similarities.append((i, similarity, chunks[i]))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)

    # æ„é€ è¿”å›ç»“æœ
    results = []
    for i in range(min(top_k, len(similarities))):
        idx, score, chunk = similarities[i]
        results.append({
            'index': idx,
            'score': score,
            'chunk': chunk
        })

    return results

def evaluate_chunk_size_performance(query, text, chunk_size, overlap_size):
    """
    è¯„ä¼°ç‰¹å®šåˆ†å—å¤§å°çš„æ€§èƒ½

    Args:
        query (str): æŸ¥è¯¢
        text (str): æ–‡æ¡£æ–‡æœ¬
        chunk_size (int): åˆ†å—å¤§å°
        overlap_size (int): é‡å å¤§å°

    Returns:
        dict: æ€§èƒ½è¯„ä¼°ç»“æœ
    """
    # åˆ›å»ºåˆ†å—
    chunks = create_chunks_with_size(text, chunk_size, overlap_size)

    # åˆ›å»ºåµŒå…¥
    embeddings = create_embeddings(chunks)

    # æ‰§è¡Œæœç´¢
    search_results = search_with_chunks(query, chunks, embeddings, top_k=3)

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    avg_similarity = np.mean([result['score'] for result in search_results])  # å¹³å‡ç›¸ä¼¼åº¦
    chunk_count = len(chunks)  # åˆ†å—æ•°é‡
    avg_chunk_length = np.mean([len(chunk) for chunk in chunks])  # å¹³å‡åˆ†å—é•¿åº¦

    # è®¡ç®—ä¸Šä¸‹æ–‡è¦†ç›–ç‡ï¼ˆtopç»“æœçš„æ€»é•¿åº¦ï¼‰
    total_context_length = sum(len(result['chunk']) for result in search_results)

    return {
        'chunk_size': chunk_size,
        'overlap_size': overlap_size,
        'chunk_count': chunk_count,
        'avg_chunk_length': avg_chunk_length,
        'avg_similarity': avg_similarity,
        'total_context_length': total_context_length,
        'search_results': search_results
    }

def compare_chunk_sizes(query, text, chunk_sizes=None):
    """
    æ¯”è¾ƒä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½

    Args:
        query (str): æŸ¥è¯¢
        text (str): æ–‡æ¡£æ–‡æœ¬
        chunk_sizes (List[int], optional): è¦æ¯”è¾ƒçš„åˆ†å—å¤§å°åˆ—è¡¨

    Returns:
        List[Dict]: å„ç§åˆ†å—å¤§å°çš„æ€§èƒ½æ¯”è¾ƒç»“æœ
    """
    # é»˜è®¤åˆ†å—å¤§å°åˆ—è¡¨
    if chunk_sizes is None:
        chunk_sizes = [400, 600, 800, 1000, 1200, 1500]

    results = []

    print(f"æ¯”è¾ƒä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½...")

    # é€ä¸€è¯„ä¼°å„ç§åˆ†å—å¤§å°çš„æ€§èƒ½
    for chunk_size in chunk_sizes:
        # è®¡ç®—é‡å å¤§å°ï¼ˆåˆ†å—å¤§å°çš„20%ï¼‰
        overlap_size = int(chunk_size * 0.2)

        # è¯„ä¼°è¯¥åˆ†å—å¤§å°çš„æ€§èƒ½
        performance = evaluate_chunk_size_performance(
            query, text, chunk_size, overlap_size
        )

        results.append(performance)

        print(f"åˆ†å—å¤§å° {chunk_size}: å¹³å‡ç›¸ä¼¼åº¦ {performance['avg_similarity']:.4f}, "
              f"åˆ†å—æ•°é‡ {performance['chunk_count']}")

    # æŒ‰å¹³å‡ç›¸ä¼¼åº¦æ’åºï¼Œæ€§èƒ½æœ€å¥½çš„æ’åœ¨å‰é¢
    results.sort(key=lambda x: x['avg_similarity'], reverse=True)

    return results

def generate_response(query, context, model="Qwen/Qwen2.5-72B-Instruct"):
    """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
    # ç³»ç»Ÿæç¤ºè¯ï¼šå®šä¹‰AIåŠ©æ‰‹çš„è¡Œä¸ºå‡†åˆ™
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä¸¥æ ¼åŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­å¾—å‡ºç­”æ¡ˆï¼Œè¯·å›ç­”ï¼š'æˆ‘æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'"

    # ç”¨æˆ·æç¤ºè¯ï¼šç»„åˆä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢
    user_prompt = f"""
    ä¸Šä¸‹æ–‡:
    {context}

    é—®é¢˜: {query}

    è¯·åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
    """

    # è°ƒç”¨å¤§æ¨¡å‹APIç”Ÿæˆå›ç­”
    response = client.chat.completions.create(
        model=model,
        temperature=0,  # æ¸©åº¦è®¾ä¸º0ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„è¾“å‡º
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )

    return response.choices[0].message.content

def adaptive_chunking_rag(pdf_path, query):
    """
    ä½¿ç”¨è‡ªé€‚åº”åˆ†å—çš„å®Œæ•´RAGæµç¨‹

    Args:
        pdf_path (str): PDFæ–‡æ¡£è·¯å¾„
        query (str): ç”¨æˆ·æŸ¥è¯¢

    Returns:
        dict: å®Œæ•´çš„å¤„ç†ç»“æœ
    """
    print("å¼€å§‹è‡ªé€‚åº”åˆ†å—RAGæµç¨‹...")

    # 1. æå–æ–‡æ¡£æ–‡æœ¬
    print("1. æå–æ–‡æ¡£æ–‡æœ¬...")
    text = extract_text_from_pdf(pdf_path)
    print(f"æ–‡æ¡£æ€»é•¿åº¦: {len(text)} å­—ç¬¦")

    # 2. åˆ†ææ–‡æ¡£ç‰¹å¾
    print("2. åˆ†ææ–‡æ¡£ç‰¹å¾...")
    doc_characteristics = analyze_document_characteristics(text)
    print(f"æ–‡æ¡£ç‰¹å¾: å¹³å‡æ®µè½é•¿åº¦={doc_characteristics['avg_paragraph_length']:.1f}, "
          f"è¯æ±‡ä¸°å¯Œåº¦={doc_characteristics['vocabulary_richness']:.3f}")

    # 3. åˆ†ææŸ¥è¯¢ç‰¹å¾
    print("3. åˆ†ææŸ¥è¯¢ç‰¹å¾...")
    query_characteristics = analyze_query_characteristics(query)
    print(f"æŸ¥è¯¢ç‰¹å¾: é•¿åº¦={query_characteristics['query_length']}, "
          f"å¤æ‚åº¦={query_characteristics['complexity_score']}")

    # 4. æ¨èæœ€ä¼˜åˆ†å—å¤§å°
    print("4. æ¨èæœ€ä¼˜åˆ†å—å¤§å°...")
    recommended_chunk_size, recommended_overlap, reason = recommend_chunk_size(
        doc_characteristics, query_characteristics
    )
    print(f"æ¨èç­–ç•¥: {reason}")

    # 5. æ¯”è¾ƒä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½
    print("5. æ¯”è¾ƒä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½...")
    comparison_results = compare_chunk_sizes(
        query, text,
        chunk_sizes=[400, 600, 800, recommended_chunk_size, 1200, 1500]
    )

    # 6. ä½¿ç”¨æœ€ä½³åˆ†å—å¤§å°è¿›è¡ŒRAG
    print("6. ä½¿ç”¨æœ€ä½³åˆ†å—å¤§å°è¿›è¡ŒRAG...")
    best_performance = comparison_results[0]
    best_chunk_size = best_performance['chunk_size']

    print(f"é€‰æ‹©æœ€ä½³åˆ†å—å¤§å°: {best_chunk_size}")

    # 7. ç”Ÿæˆæœ€ç»ˆå›ç­”
    # ç»„åˆæ£€ç´¢åˆ°çš„ç›¸å…³åˆ†å—ä½œä¸ºä¸Šä¸‹æ–‡
    context = "\n\n".join([
        f"æ®µè½{i+1}: {result['chunk']}"
        for i, result in enumerate(best_performance['search_results'])
    ])

    response = generate_response(query, context)

    return {
        'query': query,
        'doc_characteristics': doc_characteristics,
        'query_characteristics': query_characteristics,
        'recommended_chunk_size': recommended_chunk_size,
        'recommended_reason': reason,
        'comparison_results': comparison_results,
        'best_chunk_size': best_chunk_size,
        'best_performance': best_performance,
        'context': context,
        'response': response
    }

## å®é™…åº”ç”¨ç¤ºä¾‹

# è‡ªé€‚åº”åˆ†å—RAGå®Œæ•´æ¼”ç¤º
pdf_path = "../../basic_rag/data/Attention Is All You Need.pdf"
query = "transformeræ¨¡å‹åœ¨ç»è¿‡8ä¸ªGPUè®­ç»ƒ3.5å¤©ååˆ›ä¸‹çš„å•æ¨¡å‹BLEUæ–°çºªå½•æ˜¯å¤šå°‘ï¼Ÿ"

print(f"æŸ¥è¯¢: {query}")
print("="*60)

# æ‰§è¡Œè‡ªé€‚åº”åˆ†å—RAG
result = adaptive_chunking_rag(pdf_path, query)

# æ˜¾ç¤ºæ–‡æ¡£åˆ†æç»“æœ
print(f"\nğŸ“Š æ–‡æ¡£ç‰¹å¾åˆ†æ:")
doc_chars = result['doc_characteristics']
print(f"- æ€»é•¿åº¦: {doc_chars['total_length']} å­—ç¬¦")
print(f"- å¹³å‡å¥å­é•¿åº¦: {doc_chars['avg_sentence_length']:.1f} å­—ç¬¦")
print(f"- å¹³å‡æ®µè½é•¿åº¦: {doc_chars['avg_paragraph_length']:.1f} å­—ç¬¦")
print(f"- è¯æ±‡ä¸°å¯Œåº¦: {doc_chars['vocabulary_richness']:.3f}")

# æ˜¾ç¤ºæŸ¥è¯¢åˆ†æç»“æœ
print(f"\nğŸ¯ æŸ¥è¯¢ç‰¹å¾åˆ†æ:")
query_chars = result['query_characteristics']
print(f"- æŸ¥è¯¢é•¿åº¦: {query_chars['query_length']} å­—ç¬¦")
print(f"- è¯æ•°: {query_chars['word_count']}")
print(f"- å¤æ‚åº¦è¯„åˆ†: {query_chars['complexity_score']}")
print(f"- æ˜¯å¦å…·ä½“æŸ¥è¯¢: {query_chars['is_specific']}")

# æ˜¾ç¤ºæ¨èç»“æœ
print(f"\nğŸ’¡ æ¨èç­–ç•¥:")
print(f"- æ¨èåˆ†å—å¤§å°: {result['recommended_chunk_size']} å­—ç¬¦")
print(f"- æ¨èç†ç”±: {result['recommended_reason']}")

# æ˜¾ç¤ºæ€§èƒ½æ¯”è¾ƒ
print(f"\nğŸ“ˆ åˆ†å—å¤§å°æ€§èƒ½æ¯”è¾ƒ:")
print("åˆ†å—å¤§å° | å¹³å‡ç›¸ä¼¼åº¦ | åˆ†å—æ•°é‡ | å¹³å‡åˆ†å—é•¿åº¦")
print("-" * 50)
for perf in result['comparison_results'][:5]:
    print(f"{perf['chunk_size']:^8} | {perf['avg_similarity']:^10.4f} | "
          f"{perf['chunk_count']:^8} | {perf['avg_chunk_length']:^12.1f}")

print(f"\nğŸ† æœ€ä½³åˆ†å—å¤§å°: {result['best_chunk_size']} å­—ç¬¦")

# æ˜¾ç¤ºæœç´¢ç»“æœ
print(f"\nğŸ” æœç´¢ç»“æœé¢„è§ˆ:")
for i, search_result in enumerate(result['best_performance']['search_results'], 1):
    print(f"\nç»“æœ{i} (ç›¸ä¼¼åº¦: {search_result['score']:.4f}):")
    print(f"{search_result['chunk'][:200]}...")

# æ˜¾ç¤ºæœ€ç»ˆå›ç­”
print(f"\nğŸ¤– ç”Ÿæˆçš„å›ç­”:")
print(result['response'])

"""
æŸ¥è¯¢: transformeræ¨¡å‹åœ¨ç»è¿‡8ä¸ªGPUè®­ç»ƒ3.5å¤©ååˆ›ä¸‹çš„å•æ¨¡å‹BLEUæ–°çºªå½•æ˜¯å¤šå°‘ï¼Ÿ
============================================================
å¼€å§‹è‡ªé€‚åº”åˆ†å—RAGæµç¨‹...
1. æå–æ–‡æ¡£æ–‡æœ¬...
æ–‡æ¡£æ€»é•¿åº¦: 39288 å­—ç¬¦
2. åˆ†ææ–‡æ¡£ç‰¹å¾...
æ–‡æ¡£ç‰¹å¾: å¹³å‡æ®µè½é•¿åº¦=39287.0, è¯æ±‡ä¸°å¯Œåº¦=0.343
3. åˆ†ææŸ¥è¯¢ç‰¹å¾...
æŸ¥è¯¢ç‰¹å¾: é•¿åº¦=45, å¤æ‚åº¦=1
4. æ¨èæœ€ä¼˜åˆ†å—å¤§å°...
æ¨èç­–ç•¥: æ–‡æ¡£æ®µè½è¾ƒé•¿ï¼Œè¯æ±‡ç›¸å¯¹å•ä¸€ï¼ŒæŸ¥è¯¢å…·ä½“å¯ä½¿ç”¨è¾ƒå°åˆ†å—ï¼Œæ¨èåˆ†å—å¤§å°ä¸º832å­—ç¬¦
5. æ¯”è¾ƒä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½...
æ¯”è¾ƒä¸åŒåˆ†å—å¤§å°çš„æ€§èƒ½...
åˆ†å—å¤§å° 400: å¹³å‡ç›¸ä¼¼åº¦ 0.7345, åˆ†å—æ•°é‡ 123
åˆ†å—å¤§å° 600: å¹³å‡ç›¸ä¼¼åº¦ 0.7321, åˆ†å—æ•°é‡ 82
åˆ†å—å¤§å° 800: å¹³å‡ç›¸ä¼¼åº¦ 0.7169, åˆ†å—æ•°é‡ 62
åˆ†å—å¤§å° 832: å¹³å‡ç›¸ä¼¼åº¦ 0.7296, åˆ†å—æ•°é‡ 59
åˆ†å—å¤§å° 1200: å¹³å‡ç›¸ä¼¼åº¦ 0.7298, åˆ†å—æ•°é‡ 41
åˆ†å—å¤§å° 1500: å¹³å‡ç›¸ä¼¼åº¦ 0.7286, åˆ†å—æ•°é‡ 33
6. ä½¿ç”¨æœ€ä½³åˆ†å—å¤§å°è¿›è¡ŒRAG...
é€‰æ‹©æœ€ä½³åˆ†å—å¤§å°: 400

ğŸ“Š æ–‡æ¡£ç‰¹å¾åˆ†æ:
- æ€»é•¿åº¦: 39288 å­—ç¬¦
- å¹³å‡å¥å­é•¿åº¦: 69.3 å­—ç¬¦
- å¹³å‡æ®µè½é•¿åº¦: 39287.0 å­—ç¬¦
- è¯æ±‡ä¸°å¯Œåº¦: 0.343

ğŸ¯ æŸ¥è¯¢ç‰¹å¾åˆ†æ:
- æŸ¥è¯¢é•¿åº¦: 45 å­—ç¬¦
- è¯æ•°: 1
- å¤æ‚åº¦è¯„åˆ†: 1
- æ˜¯å¦å…·ä½“æŸ¥è¯¢: True

ğŸ’¡ æ¨èç­–ç•¥:
- æ¨èåˆ†å—å¤§å°: 832 å­—ç¬¦
- æ¨èç†ç”±: æ–‡æ¡£æ®µè½è¾ƒé•¿ï¼Œè¯æ±‡ç›¸å¯¹å•ä¸€ï¼ŒæŸ¥è¯¢å…·ä½“å¯ä½¿ç”¨è¾ƒå°åˆ†å—ï¼Œæ¨èåˆ†å—å¤§å°ä¸º832å­—ç¬¦

ğŸ“ˆ åˆ†å—å¤§å°æ€§èƒ½æ¯”è¾ƒ:
åˆ†å—å¤§å° | å¹³å‡ç›¸ä¼¼åº¦ | åˆ†å—æ•°é‡ | å¹³å‡åˆ†å—é•¿åº¦
--------------------------------------------------
  400    |   0.7345   |   123    |    398.8    
  600    |   0.7321   |    82    |    597.7    
  1200   |   0.7298   |    41    |    1192.4   
  832    |   0.7296   |    59    |    829.1    
  1500   |   0.7286   |    33    |    1481.5   

ğŸ† æœ€ä½³åˆ†å—å¤§å°: 400 å­—ç¬¦

ğŸ” æœç´¢ç»“æœé¢„è§ˆ:

ç»“æœ1 (ç›¸ä¼¼åº¦: 0.7691):
e training time, the number of GPUs used, and an estimate of the sustained
single-precision ï¬‚oating-point capacity of each GPU 5.
6.2
Model Variations
To evaluate the importance of different component...

ç»“æœ2 (ç›¸ä¼¼åº¦: 0.7218):
 The conï¬guration of this model is
listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fracti...

ç»“æœ3 (ç›¸ä¼¼åº¦: 0.7127):
iï¬cantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT...

ğŸ¤– ç”Ÿæˆçš„å›ç­”:
æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒTransformeræ¨¡å‹åœ¨ç»è¿‡8ä¸ªGPUè®­ç»ƒ3.5å¤©åï¼Œåœ¨WMT 2014 English-to-Frenchç¿»è¯‘ä»»åŠ¡ä¸Šåˆ›ä¸‹çš„å•æ¨¡å‹BLEUæ–°çºªå½•æ˜¯41.8ã€‚

"""