{
 "cells": [
  {
   "attachments": {
    "d9d5305c-e5bb-4934-b91d-5988c87fd767.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAOBCAYAAACwNGoqAAAMP2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBCCSAgJfQmCEgJICWEFkB6EWyEJEAoMQaCiB1dVHDtYgEbuiqi2AGxI3YWwd4XRRSUdbFgV96kgK77yvfO9829//3nzH/OnDu3DADqp7hicQ6qAUCuKF8SGxLAGJucwiB1AwTggAYIgMDl5YlZ0dERANrg+e/27ib0hnbNQab1z/7/app8QR4PACQa4jR+Hi8X4kMA4JU8sSQfAKKMN5+aL5Zh2IC2BCYI8UIZzlDgShlOU+B9cp/4WDbEzQCoqHG5kgwAaG2QZxTwMqAGrQ9iJxFfKAJAnQGxb27uZD7EqRDbQB8xxDJ9ZtoPOhl/00wb0uRyM4awYi5yUwkU5olzuNP+z3L8b8vNkQ7GsIJNLVMSGiubM6zb7ezJ4TKsBnGvKC0yCmItiD8I+XJ/iFFKpjQ0QeGPGvLy2LBmQBdiJz43MBxiQ4iDRTmREUo+LV0YzIEYrhC0UJjPiYdYD+KFgrygOKXPZsnkWGUstC5dwmYp+QtciTyuLNZDaXYCS6n/OlPAUepjtKLM+CSIKRBbFAgTIyGmQeyYlx0XrvQZXZTJjhz0kUhjZflbQBwrEIUEKPSxgnRJcKzSvzQ3b3C+2OZMISdSiQ/kZ8aHKuqDNfO48vzhXLA2gYiVMKgjyBsbMTgXviAwSDF3rFsgSohT6nwQ5wfEKsbiFHFOtNIfNxPkhMh4M4hd8wrilGPxxHy4IBX6eLo4PzpekSdelMUNi1bkgy8DEYANAgEDSGFLA5NBFhC29tb3witFTzDgAgnIAALgoGQGRyTJe0TwGAeKwJ8QCUDe0LgAea8AFED+6xCrODqAdHlvgXxENngKcS4IBznwWiofJRqKlgieQEb4j+hc2Hgw3xzYZP3/nh9kvzMsyEQoGelgRIb6oCcxiBhIDCUGE21xA9wX98Yj4NEfNheciXsOzuO7P+EpoZ3wmHCD0EG4M0lYLPkpyzGgA+oHK2uR9mMtcCuo6YYH4D5QHSrjurgBcMBdYRwW7gcju0GWrcxbVhXGT9p/m8EPd0PpR3Yio+RhZH+yzc8jaXY0tyEVWa1/rI8i17asherOHen6Oz/6h+nx4Dv/ZE1uIHcTOY6exi9gxrB4wsJNYA9aCHZfhodX1RL66BqPFyvPJhjrCf8QbvLOySuY51Tj1OH1R9OULCmXvaMCeLJ4mEWZk5jNY8IsgYHBEPMcRDBcnF1cAZN8XxevrTYz8u4Hotnzn5v0BgM/JgYGBo9+5sJMA7PeAj/+R75wNE346VAG4cIQnlRQoOFx2IMC3hDp80vSBMTAHNnA+LsAdeAN/EATCQBSIB8lgIsw+E65zCZgKZoC5oASUgWVgNVgPNoGtYCfYAw6AenAMnAbnwGXQBm6Ae3D1dIEXoA+8A58RBCEhVISO6CMmiCVij7ggTMQXCUIikFgkGUlFMhARIkVmIPOQMmQFsh7ZglQj+5EjyGnkItKO3EEeIT3Ia+QTiqFqqDZqhFqhI1EmykLD0Xh0ApqBTkGL0PnoEnQtWoXuRuvQ0+hl9Abagb5A+zGAqWK6mCnmgDExNhaFpWDpmASbhZVi5VgVVos1wvt8DevAerGPOBGn4wzcAa7gUDwB5+FT8Fn4Ynw9vhOvw5vxa/gjvA//RqASDAn2BC8ChzCWkEGYSighlBO2Ew4TzsJnqYvwjkgk6hKtiR7wWUwmZhGnExcTNxD3Ek8R24mdxH4SiaRPsif5kKJIXFI+qYS0jrSbdJJ0ldRF+qCiqmKi4qISrJKiIlIpVilX2aVyQuWqyjOVz2QNsiXZixxF5pOnkZeSt5EbyVfIXeTPFE2KNcWHEk/JosylrKXUUs5S7lPeqKqqmql6qsaoClXnqK5V3ad6QfWR6kc1LTU7NbbaeDWp2hK1HWqn1O6ovaFSq[... 134560 chars omitted ...]
    }
   },
   "cell_type": "markdown",
   "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0",
   "metadata": {},
   "source": [
    "## Part 5: Multi Query\n",
    "\n",
    "Flow:\n",
    "\n",
    "![Screenshot 2024-02-12 at 12.39.59 PM.png](attachment:9efe017a-075f-4017-abef-174c755b11c6.png)\n",
    "\n",
    "Docs:\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
    "\n",
    "### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_query_explanation",
   "metadata": {},
   "source": [
    "### 多查询检索 (Multi Query) 技术说明\n",
    "\n",
    "多查询检索是一种通过生成多个相关查询来提高检索效果的技术。其核心思想是：\n",
    "\n",
    "1. **问题背景**: 用户的原始查询可能不够精确或存在歧义，导致检索结果不理想\n",
    "2. **解决方案**: 使用LLM生成多个不同角度的查询变体\n",
    "3. **技术优势**: \n",
    "   - 提高检索覆盖率\n",
    "   - 克服单一查询的局限性\n",
    "   - 从不同角度获取相关信息\n",
    "4. **应用场景**: 适用于复杂问题、模糊查询或需要全面信息的场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 索引构建 (INDEXING) ####\n",
    "\n",
    "# 加载博客文章\n",
    "# 使用WebBaseLoader从指定URL加载文档\n",
    "# bs_kwargs参数配置BeautifulSoup解析器，只提取特定CSS类的内容\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# 文档分割\n",
    "# 使用RecursiveCharacterTextSplitter将长文档分割成小块\n",
    "# chunk_size=300: 每个文本块的最大字符数\n",
    "# chunk_overlap=50: 相邻文本块之间的重叠字符数，保持上下文连贯性\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# 执行分割\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# 向量索引构建\n",
    "# 使用OpenAI的嵌入模型将文本转换为向量\n",
    "# 使用ChromaDB作为向量存储数据库\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 创建检索器\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa",
   "metadata": {},
   "source": [
    "### 多查询提示词设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 多查询生成提示词模板\n",
    "# 该提示词指导LLM从5个不同角度重新表述用户问题\n",
    "# 目的是克服基于距离的相似性搜索的局限性\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 构建多查询生成链\n",
    "# 1. 使用提示词模板\n",
    "# 2. 调用ChatOpenAI生成查询\n",
    "# 3. 解析输出为字符串\n",
    "# 4. 按换行符分割成查询列表\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique_union_explanation",
   "metadata": {},
   "source": [
    "### 文档去重与合并策略\n",
    "\n",
    "在多查询检索中，不同查询可能返回相同的文档，需要去重处理：\n",
    "\n",
    "1. **问题**: 多个查询可能检索到重复的文档\n",
    "2. **解决方案**: 使用唯一并集函数去除重复文档\n",
    "3. **实现方法**: \n",
    "   - 将所有检索结果扁平化\n",
    "   - 使用JSON序列化作为唯一标识\n",
    "   - 利用集合(set)去重\n",
    "   - 反序列化回文档对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f253520-386f-434b-8daa-d6dadb89eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" 获取文档的唯一并集 \"\"\"\n",
    "    # 扁平化列表：将嵌套的文档列表转换为单层列表\n",
    "    # dumps将文档对象序列化为字符串，便于比较和去重\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    \n",
    "    # 使用set去重，获取唯一文档\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    \n",
    "    # 反序列化回文档对象\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# 执行多查询检索\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# 构建检索链：生成查询 -> 并行检索 -> 去重合并\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_rag_chain_explanation",
   "metadata": {},
   "source": [
    "### 最终RAG链构建\n",
    "\n",
    "将多查询检索与答案生成结合：\n",
    "\n",
    "1. **输入处理**: 同时处理上下文和问题\n",
    "2. **上下文准备**: 使用多查询检索获取相关文档\n",
    "3. **答案生成**: 基于检索到的文档生成答案\n",
    "4. **输出解析**: 将LLM输出解析为字符串格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e74e8-ddae-4165-9e4b-0022ac125194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG提示词模板\n",
    "# 简洁明了的问答格式，指导LLM基于上下文回答问题\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 构建最终RAG链\n",
    "# 使用itemgetter获取问题字段，确保数据正确传递\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 执行RAG查询\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "attachments": {
    "0bc49f5b-c338-4cd4-ac04-8744994e0e81.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAB3gAAAIeCAYAAABZbd4/AAAMP2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBCCSAgJfQmCEgJICWEFkB6EWyEJEAoMQaCiB1dVHDtYgEbuiqi2AGxI3YWwd4XRRSUdbFgV96kgK77yvfO9829//3nzH/OnDu3DADqp7hicQ6qAUCuKF8SGxLAGJucwiB1AwTggAYIgMDl5YlZ0dERANrg+e/27ib0hnbNQab1z/7/app8QR4PACQa4jR+Hi8X4kMA4JU8sSQfAKKMN5+aL5Zh2IC2BCYI8UIZzlDgShlOU+B9cp/4WDbEzQCoqHG5kgwAaG2QZxTwMqAGrQ9iJxFfKAJAnQGxb27uZD7EqRDbQB8xxDJ9ZtoPOhl/00wb0uRyM4awYi5yUwkU5olzuNP+z3L8b8vNkQ7GsIJNLVMSGiubM6zb7ezJ4TKsBnGvKC0yCmItiD8I+XJ/iFFKpjQ0QeGPGvLy2LBmQBdiJz43MBxiQ4iDRTmREUo+LV0YzIEYrhC0UJjPiYdYD+KFgrygOKXPZsnkWGUstC5dwmYp+QtciTyuLNZDaXYCS6n/OlPAUepjtKLM+CSIKRBbFAgTIyGmQeyYlx0XrvQZXZTJjhz0kUhjZflbQBwrEIUEKPSxgnRJcKzSvzQ3b3C+2OZMISdSiQ/kZ8aHKuqDNfO48vzhXLA2gYiVMKgjyBsbMTgXviAwSDF3rFsgSohT6nwQ5wfEKsbiFHFOtNIfNxPkhMh4M4hd8wrilGPxxHy4IBX6eLo4PzpekSdelMUNi1bkgy8DEYANAgEDSGFLA5NBFhC29tb3witFTzDgAgnIAALgoGQGRyTJe0TwGAeKwJ8QCUDe0LgAea8AFED+6xCrODqAdHlvgXxENngKcS4IBznwWiofJRqKlgieQEb4j+hc2Hgw3xzYZP3/nh9kvzMsyEQoGelgRIb6oCcxiBhIDCUGE21xA9wX98Yj4NEfNheciXsOzuO7P+EpoZ3wmHCD0EG4M0lYLPkpyzGgA+oHK2uR9mMtcCuo6YYH4D5QHSrjurgBcMBdYRwW7gcju0GWrcxbVhXGT9p/m8EPd0PpR3Yio+RhZH+yzc8jaXY0tyEVWa1/rI8i17asherOHen6Oz/6h+nx4Dv/ZE1uIHcTOY6exi9gxrB4wsJNYA9aCHZfhodX1RL66BqPFyvPJhjrCf8QbvLOySuY51Tj1OH1R9OULCmXvaMCeLJ4mEWZk5jNY8IsgYHBEPMcRDBcnF1cAZN8XxevrTYz8u4Hotnzn5v0BgM/JgYGBo9+5sJMA7PeAj/+R75wNE346VAG4cIQnlRQoOFx2IMC3hDp80vSBMTAHNnA+LsAdeAN/EATCQBSIB8lgIsw+E65zCZgKZoC5oASUgWVgNVgPNoGtYCfYAw6AenAMnAbnwGXQBm6Ae3D1dIEXoA+8A58RBCEhVISO6CMmiCVij7ggTMQXCUIikFgkGUlFMhARIkVmIPOQMmQFsh7ZglQj+5EjyGnkItKO3EEeIT3Ia+QTiqFqqDZqhFqhI1EmykLD0Xh0ApqBTkGL0PnoEnQtWoXuRuvQ0+hl9Abagb5A+zGAqWK6mCnmgDExNhaFpWDpmASbhZVi5VgVVos1wvt8DevAerGPOBGn4wzcAa7gUDwB5+FT8Fn4Ynw9vhOvw5vxa/gjvA//RqASDAn2BC8ChzCWkEGYSighlBO2Ew4TzsJnqYvwjkgk6hKtiR7wWUwmZhGnExcTNxD3Ek8R24mdxH4SiaRPsif5kKJIXFI+qYS0jrSbdJJ0ldRF+qCiqmKi4qISrJKiIlIpVilX2aVyQuWqyjOVz2QNsiXZixxF5pOnkZeSt5EbyVfIXeTPFE2KNcWHEk/JosylrKXUUs5S7lPeqKqqmql6qsaoClXnqK5V3ad6QfWR6kc1LTU7NbbaeDWp2hK1HWqn1O6ovaFSq[... 417064 chars omitted ...]
    }
   },
   "cell_type": "markdown",
   "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0",
   "metadata": {},
   "source": [
    "## Part 5: Multi Query\n",
    "\n",
    "Flow:\n",
    "\n",
    "![Screenshot 2024-02-12 at 12.39.59 PM.png](attachment:9efe017a-075f-4017-abef-174c755b11c6.png)\n",
    "\n",
    "Docs:\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
    "\n",
    "### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag_fusion_explanation",
   "metadata": {},
   "source": [
    "## Part 6: RAG-Fusion\n",
    "\n",
    "RAG-Fusion是一种结合了多查询生成和倒数排名融合(RRF)的高级检索技术。\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "1. **多查询生成**: 为单个问题生成多个相关查询\n",
    "2. **并行检索**: 对每个查询执行独立的向量检索\n",
    "3. **倒数排名融合**: 使用RRF算法合并多个检索结果\n",
    "4. **重新排序**: 基于融合分数对文档进行最终排序\n",
    "\n",
    "### 技术优势\n",
    "\n",
    "- **提高召回率**: 通过多查询覆盖更多相关内容\n",
    "- **增强鲁棒性**: RRF算法减少对单一查询的依赖\n",
    "- **改善排序质量**: 综合考虑多个检索结果的排名\n",
    "\n",
    "### 应用场景\n",
    "\n",
    "- 复杂问题的多角度检索\n",
    "- 需要高召回率的搜索场景\n",
    "- 查询扩展和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag_fusion_prompt_design",
   "metadata": {},
   "source": [
    "### RAG-Fusion提示词设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7075b-b80d-461d-9e2e-e05e29436f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion查询生成提示词\n",
    "# 与多查询不同，这里明确要求生成\"相关\"查询，而非不同角度的查询\n",
    "# 生成4个查询，用于后续的RRF融合\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\\n",
    "\\n\n",
    "Generate multiple search queries related to: {question} \\\n",
    "\\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781b40c-c408-42f4-ae14-cd11be513b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 构建RAG-Fusion查询生成链\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrf_algorithm_explanation",
   "metadata": {},
   "source": [
    "### 倒数排名融合(RRF)算法详解\n",
    "\n",
    "RRF是一种将多个排名列表融合为单一排名的算法：\n",
    "\n",
    "#### 算法原理\n",
    "\n",
    "对于每个文档，RRF分数计算公式：\n",
    "```\n",
    "RRF_score = Σ(1 / (rank + k))\n",
    "```\n",
    "\n",
    "其中：\n",
    "- `rank`: 文档在单个列表中的排名位置\n",
    "- `k`: 平滑参数，通常设为60\n",
    "- `Σ`: 对所有列表中该文档的分数求和\n",
    "\n",
    "#### 算法特点\n",
    "\n",
    "1. **无需调参**: k参数对结果影响较小\n",
    "2. **鲁棒性强**: 不依赖于具体的评分机制\n",
    "3. **计算简单**: 易于实现和理解\n",
    "4. **效果稳定**: 在多种场景下表现良好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1adff1-e993-4747-b95d-656eaaeccfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    倒数排名融合函数\n",
    "    \n",
    "    参数:\n",
    "        results: 多个检索结果列表的列表\n",
    "        k: RRF公式中的平滑参数，默认60\n",
    "    \n",
    "    返回:\n",
    "        融合后的文档列表，按分数降序排列\n",
    "    \"\"\"\n",
    "    # 初始化字典存储每个文档的融合分数\n",
    "    fused_scores = {}\n",
    "\n",
    "    # 遍历每个检索结果列表\n",
    "    for docs in results:\n",
    "        # 遍历列表中的每个文档及其排名\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # 将文档序列化为字符串作为唯一键\n",
    "            doc_str = dumps(doc)\n",
    "            \n",
    "            # 如果文档还未在字典中，初始化分数为0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            \n",
    "            # 获取当前分数并更新\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # 使用RRF公式计算新分数：1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # 按融合分数降序排序文档\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "# 构建RAG-Fusion检索链\n",
    "# 生成查询 -> 并行检索 -> RRF融合\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2adf2d-3d9f-4d43-afb0-8304edcfb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG提示词模板\n",
    "# 注意：这里使用融合后的文档作为上下文\n",
    "# 需要提取文档内容而不仅仅是文档对象\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建最终RAG链\n",
    "# 注意：RRF返回的是(doc, score)元组列表，需要提取文档内容\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decomposition_explanation",
   "metadata": {},
   "source": [
    "## Part 7: Decomposition (问题分解)\n",
    "\n",
    "问题分解是一种将复杂问题拆分为多个简单子问题的技术。\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "1. **分而治之**: 将复杂问题分解为可独立解决的子问题\n",
    "2. **逐步求解**: 分别回答每个子问题，然后综合答案\n",
    "3. **降低难度**: 简化检索和推理过程\n",
    "\n",
    "### 应用场景\n",
    "\n",
    "- **多步骤推理**: 需要逻辑链条的复杂问题\n",
    "- **广泛主题**: 涵盖多个子主题的问题\n",
    "- **详细分析**: 需要深入探讨的复杂议题\n",
    "\n",
    "### 实现步骤\n",
    "\n",
    "1. **问题分析**: 识别问题的主要组成部分\n",
    "2. **子问题生成**: 为每个组成部分创建具体问题\n",
    "3. **独立检索**: 为每个子问题执行专门的检索\n",
    "4. **答案综合**: 合并所有子问题的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82fac99-58dc-4bb9-84e6-51180db855ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 问题分解提示词模板\n",
    "# 指导LLM将复杂问题分解为3个相关的子问题\n",
    "# 每个子问题应该能够独立回答，共同构成完整答案\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\\n",
    "\\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\\n",
    "Generate multiple search queries related to: {question} \\\n",
    "\\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM配置\n",
    "# 使用temperature=0确保输出的一致性和确定性\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 构建问题分解链\n",
    "# 提示词 -> LLM -> 输出解析 -> 分割成列表\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# 执行问题分解\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is LLM technology and how does it work in autonomous agent systems?',\n",
       " '2. What are the specific components that make up an LLM-powered autonomous agent system?',\n",
       " '3. How do the main components of an LLM-powered autonomous agent system interact with each other to enable autonomous functionality?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看分解后的子问题\n",
    "# 可以看到复杂问题被分解为3个层次化的子问题\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyde_explanation",
   "metadata": {},
   "source": [
    "## Part 8: Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "HyDE是一种创新的检索技术，通过生成假设文档来提高检索效果。\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "1. **假设文档生成**: 基于查询生成一个假设的答案文档\n",
    "2. **嵌入匹配**: 使用假设文档的嵌入向量进行检索\n",
    "3. **桥接作用**: 连接查询语义与文档语义之间的差距\n",
    "\n",
    "### 技术优势\n",
    "\n",
    "- **语义增强**: 生成文档提供更丰富的语义信息\n",
    "- **查询扩展**: 隐含地扩展了原始查询\n",
    "- **领域适应**: 生成的假设文档可以适应特定领域\n",
    "\n",
    "### 实现流程\n",
    "\n",
    "1. **查询分析**: 理解用户查询的意图\n",
    "2. **假设生成**: 生成可能的答案文档\n",
    "3. **嵌入计算**: 计算假设文档的向量嵌入\n",
    "4. **向量检索**: 使用假设文档嵌入进行相似性搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyde_prompt_design",
   "metadata": {},
   "source": [
    "### HyDE提示词设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyde_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# HyDE提示词模板\n",
    "# 指导LLM生成一个假设的、信息丰富的文档来回答问题\n",
    "# 这个假设文档将用于后续的向量检索\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建HyDE链\n",
    "# 生成假设文档 -> 解析输出\n",
    "generate_hyde = (\n",
    "    prompt_hyde \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "# 测试HyDE生成\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "hypothetical_doc = generate_hyde.invoke({\"question\": question})\n",
    "\n",
    "print(\"=== 生成的假设文档 ===\")\n",
    "print(hypothetical_doc[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyde_retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用假设文档进行检索\n",
    "# 将生成的假设文档作为查询向量进行相似性搜索\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 注意：这里需要重新初始化向量存储或使用已有的向量存储\n",
    "# 假设我们已经有了向量存储和检索器\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 使用假设文档进行检索\n",
    "# docs = retriever.invoke(hypothetical_doc)\n",
    "# print(f\"检索到 {len(docs)} 个相关文档\")\n",
    "\n",
    "# 完整的HyDE RAG链\n",
    "hyde_rag_chain = (\n",
    "    {\"context\": generate_hyde | retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# result = hyde_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stepback_explanation",
   "metadata": {},
   "source": [
    "## Part 9: Step-back Questioning (后退提问)\n",
    "\n",
    "Step-back questioning是一种通过抽象化具体问题来获得更广泛上下文的技术。\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "1. **问题抽象**: 将具体问题提升到更抽象的层次\n",
    "2. **上下文扩展**: 获取更广泛的背景信息\n",
    "3. **逐步推理**: 从抽象到具体，逐步回答问题\n",
    "\n",
    "### 实现原理\n",
    "\n",
    "- **第一步**: 生成一个更抽象的\"后退\"问题\n",
    "- **第二步**: 检索抽象问题的相关信息\n",
    "- **第三步**: 结合抽象和具体信息生成最终答案\n",
    "\n",
    "### 应用场景\n",
    "\n",
    "- **需要背景知识**: 当具体问题需要更广泛的上下文时\n",
    "- **复杂推理**: 涉及多个层次的概念和关系\n",
    "- **概念解释**: 需要解释基本原理或背景概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stepback_prompt_design",
   "metadata": {},
   "source": [
    "### Step-back提示词设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stepback_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step-back问题生成提示词\n",
    "# 指导LLM生成一个更抽象、更通用的后退问题\n",
    "# 后退问题应该能够提供更广泛的上下文和背景知识\n",
    "template = \"\"\"You are an expert at world knowledge. Your task is to step back and \n",
    "abstract the question to a more generic step-back question that would help\n",
    "provide more context and information for answering the specific question.\n",
    "\n",
    "Specific question: {question}\n",
    "\n",
    "Step-back question:\"\"\"\n",
    "\n",
    "prompt_stepback = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建Step-back问题生成链\n",
    "generate_stepback = (\n",
    "    prompt_stepback \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "# 测试Step-back问题生成\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "stepback_question = generate_stepback.invoke({\"question\": question})\n",
    "\n",
    "print(f\"原始问题: {question}\")\n",
    "print(f\"Step-back问题: {stepback_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stepback_rag_chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建完整的Step-back RAG链\n",
    "# 包含两个检索路径：原始问题和Step-back问题\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# 并行检索原始问题和Step-back问题的相关信息\n",
    "parallel_retrieval = RunnableParallel(\n",
    "    context_original=retriever,\n",
    "    context_stepback=generate_stepback | retriever\n",
    ")\n",
    "\n",
    "# Step-back RAG提示词\n",
    "# 结合抽象和具体上下文来生成答案\n",
    "template = \"\"\"Answer the question based on the following contexts:\n",
    "\n",
    "Specific context:\n",
    "{context_original}\n",
    "\n",
    "General context (from step-back question):\n",
    "{context_stepback}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_stepback_rag = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建最终的Step-back RAG链\n",
    "stepback_rag_chain = (\n",
    "    {\"context_original\": retriever, \"context_stepback\": generate_stepback | retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_stepback_rag\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 执行Step-back RAG\n",
    "# result = stepback_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_comparison",
   "metadata": {},
   "source": [
    "## 技术总结与对比\n",
    "\n",
    "### 查询转换技术对比\n",
    "\n",
    "| 技术 | 核心思想 | 优势 | 适用场景 | 实现复杂度 |\n",
    "|------|----------|------|----------|------------|\n",
    "| **Multi Query** | 多角度查询生成 | 提高覆盖率 | 复杂/模糊查询 | 中等 |\n",
    "| **RAG-Fusion** | 多查询+RRF融合 | 改善排序质量 | 高召回率需求 | 高 |\n",
    "| **Decomposition** | 问题分解 | 简化复杂问题 | 多步骤推理 | 中等 |\n",
    "| **HyDE** | 假设文档生成 | 语义增强 | 语义差距大 | 中等 |\n",
    "| **Step-back** | 抽象化提问 | 获取背景知识 | 需要上下文 | 中等 |\n",
    "\n",
    "### 选择建议\n",
    "\n",
    "1. **基础场景**: 从Multi Query开始，简单易实现\n",
    "2. **高要求场景**: 使用RAG-Fusion获得最佳效果\n",
    "3. **复杂推理**: 结合Decomposition和Step-back\n",
    "4. **语义差距**: 考虑使用HyDE桥接语义差异\n",
    "\n",
    "### 最佳实践\n",
    "\n",
    "- **组合使用**: 可以根据需求组合多种技术\n",
    "- **评估测试**: 针对具体场景测试不同技术的效果\n",
    "- **性能考虑**: 多查询技术会增加计算成本\n",
    "- **质量监控**: 监控生成查询的质量和相关性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}